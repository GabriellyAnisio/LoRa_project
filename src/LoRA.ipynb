{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning de um Modelo de Linguagem com LoRA\n",
    "\n",
    "Este notebook descreve o processo de finetuning de um modelo de linguagem utilizando a técnica LoRA (Low-Rank Adaptation). A técnica LoRA é uma abordagem que permite adaptar grandes modelos de linguagem sem a necessidade de treinar todos os parâmetros do modelo, tornando o processo mais eficiente em termos de tempo e recursos computacionais. Vamos seguir passo a passo desde a instalação das bibliotecas necessárias até o treinamento e inferência do modelo.\n",
    "\n",
    "## Instalação de Bibliotecas\n",
    "\n",
    "Primeiro, precisamos instalar as bibliotecas essenciais para o nosso projeto. Utilizaremos `bitsandbytes` para aceleração em GPU, `datasets` para carregar conjuntos de dados pré-existentes, `accelerate` para facilitar o uso de múltiplas GPUs, e `loralib` para implementar a técnica LoRA. Adicionalmente, instalamos `transformers` (a principal biblioteca para trabalhar com modelos de linguagem da Hugging Face), `torch`, e `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.45.0.dev0)\n",
      "Requirement already satisfied: torch in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.16.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabya\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gabya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
    "!pip install transformers torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autenticação no Hugging Face Hub\n",
    "\n",
    "A Hugging Face oferece uma plataforma para hospedar e compartilhar modelos de aprendizado de máquina. Precisamos autenticar nossa conta para carregar ou salvar modelos. A função notebook_login facilita esse processo de login diretamente do notebook. Para qualquer eventual dúvida com esta parte, [acesse](https://huggingface.co/docs/hub/security-tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46420d4173c6422097f431a4a70b7cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificação de GPUs Disponíveis\n",
    "\n",
    "Em seguida, verificamos as GPUs disponíveis no sistema com o comando `nvidia-smi -L`. Isso nos permite confirmar que temos os recursos de hardware necessários para treinar nosso modelo de maneira eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1650 (UUID: GPU-eec28b89-0dd4-f4e7-4720-caa347da1457)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração do Ambiente e Carregamento do Modelo\n",
    "\n",
    "Agora configuramos o ambiente de execução para usar uma GPU específica (neste caso, a GPU 0) e carregamos o modelo `bloom-560m`, que é um modelo de linguagem treinado pela BigScience. Utilizamos a função `AutoModelForCausalLM.from_pretrained` para carregar o modelo pré-treinado, e `AutoTokenizer.from_pretrained` para carregar o tokenizador correspondente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5684fbacf94809a761393f0fd0c5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gabya\\.cache\\huggingface\\hub\\models--bigscience--bloom-560m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b95d560089e4ed4ab0d74a309de611b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15b71cd23044721805754f164ab13c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/76/61/766109d3d0b4ee837dfedfab0527ead28aab8af261c2eacbb3c7f7e5d3676920/a8702498162c95d68d2724e7f333c83d7be08de81cfc091455c38730682116d3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1723578073&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzU3ODA3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83Ni82MS83NjYxMDlkM2QwYjRlZTgzN2RmZWRmYWIwNTI3ZWFkMjhhYWI4YWYyNjFjMmVhY2JiM2M3ZjdlNWQzNjc2OTIwL2E4NzAyNDk4MTYyYzk1ZDY4ZDI3MjRlN2YzMzNjODNkN2JlMDhkZTgxY2ZjMDkxNDU1YzM4NzMwNjgyMTE2ZDM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=mV1TvGv4CabQlnm25NxYDgxQJIvcFxsDB5HCUspNSJxfZaE%7E4HL4TiWRLkU-B8aBM3abgNZiqsX4jn48t7qCwfDBwHc0ppzXNDoCDFPiwzuEAoBA0GiMw2tzQdZTIdTFcWp3tylcGusgkMY%7Ex0vKVR4bBdi1fua0KPOAJXIYao3JNOll0pt6SQREmBOk4aTAOIH4zzrYahjJnH1SBxC7pcrdSntQDaffAdN121agr1fC3c27%7Ewp0SoYANi-o%7EGtoNXe%7Eal0JHhVsYmWNs9anabYqDqP9btMAg5juccjry4yXRd8ngQD6-3vQGYGGIKbGX6mO1hyYkkcYd97hGXWYww__&Key-Pair-Id=K3ESJI6DHPFC7: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8e129f0df24ec9a0b5999e786555bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  32%|###1      | 357M/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigscience/bloom-560m\",\n",
    "    device_map='auto',  \n",
    "    force_download=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BloomForCausalLM(\n",
      "  (transformer): BloomModel(\n",
      "    (word_embeddings): Embedding(250880, 1024)\n",
      "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x BloomBlock(\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): BloomAttention(\n",
      "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): BloomMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (gelu_impl): BloomGelu()\n",
      "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congelamento de Parâmetros do Modelo\n",
    "\n",
    "Para evitar o treinamento de todo o modelo, o que seria computacionalmente caro, congelamos a maioria dos parâmetros utilizando `param.requires_grad = False`. Dessa forma, apenas algumas camadas específicas serão treinadas. Também convertemos alguns parâmetros menores para o formato `float32` para melhorar a estabilidade durante o treinamento. Além disso, habilitamos o checkpointing de gradiente para reduzir o uso de memória durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False \n",
    "  if param.ndim == 1:\n",
    "    param.data = param.data.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()  \n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  '''\n",
    "  This class's forward method takes in an input and converts it to float32 form.\n",
    "  '''\n",
    "  def forward(self, x):\n",
    "    return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de Parâmetros Treináveis\n",
    "\n",
    "Para entender melhor a quantidade de parâmetros que serão ajustados durante o treinamento, criamos uma função que imprime o número total de parâmetros treináveis e a porcentagem desses em relação ao total de parâmetros do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração e Aplicação de LoRA\n",
    "\n",
    "O próximo passo é configurar e aplicar a técnica LoRA ao nosso modelo. LoRA permite que ajustemos apenas algumas partes do modelo, como cabeças de atenção, para melhorar a eficiência do treinamento. Definimos uma configuração utilizando `LoraConfig`, especificando os módulos alvo que queremos ajustar, a quantidade de dropout a ser aplicada, e outros parâmetros específicos da LoRA. Em seguida, aplicamos essa configuração ao nosso modelo com `get_peft_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 560787456 || trainable%: 0.2804741766549072\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento e Preparação dos Dados\n",
    "\n",
    "Para treinar nosso modelo, precisamos de um conjunto de dados. Neste exemplo, utilizamos um dataset de citações em inglês, que carregamos utilizando a função `load_dataset` da biblioteca `datasets`. Modificamos o dataset para adicionar uma coluna de \"predição\", que é uma combinação da citação com suas tags, e, em seguida, tokenizamos o texto para preparar os dados para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['quote', 'author', 'tags'],\n",
      "        num_rows: 2508\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['quote', 'author', 'tags'],\n",
      "    num_rows: 2508\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(data[\"train\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"“Be yourself; everyone else is already taken.” ->: ['be-yourself', 'gilbert-perreira', 'honesty', 'inspirational', 'misattributed-oscar-wilde', 'quote-investigator']\",\n",
       " \"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.” ->: ['best', 'life', 'love', 'mistakes', 'out-of-control', 'truth', 'worst']\",\n",
       " \"“Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.” ->: ['human-nature', 'humor', 'infinity', 'philosophy', 'science', 'stupidity', 'universe']\",\n",
       " \"“So many books, so little time.” ->: ['books', 'humor']\",\n",
       " \"“A room without books is like a body without a soul.” ->: ['books', 'simile', 'soul']\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_columns(example):\n",
    "    example[\"prediction\"] = example[\"quote\"] + \" ->: \" + str(example[\"tags\"])\n",
    "    return example\n",
    "\n",
    "train_dataset = data['train']\n",
    "train_dataset = train_dataset.map(merge_columns)\n",
    "train_dataset[\"prediction\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quote': '“Be yourself; everyone else is already taken.”',\n",
       " 'author': 'Oscar Wilde',\n",
       " 'tags': ['be-yourself',\n",
       "  'gilbert-perreira',\n",
       "  'honesty',\n",
       "  'inspirational',\n",
       "  'misattributed-oscar-wilde',\n",
       "  'quote-investigator'],\n",
       " 'prediction': \"“Be yourself; everyone else is already taken.” ->: ['be-yourself', 'gilbert-perreira', 'honesty', 'inspirational', 'misattributed-oscar-wilde', 'quote-investigator']\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda samples: tokenizer(samples['prediction']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['quote', 'author', 'tags', 'prediction', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2508\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração e Treinamento do Modelo\n",
    "\n",
    "Agora estamos prontos para configurar o treinamento do modelo. Utilizamos a classe `Trainer` da Hugging Face, que facilita o processo de treinamento. Definimos parâmetros como o tamanho do batch, o número de passos de acumulação de gradiente, o número de passos de aquecimento, e a taxa de aprendizado. Para minimizar o uso de recursos, configuramos o treinamento para rodar apenas por um pequeno número de passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67cd8f91ef74fb7befd3dbb5e598650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9635, 'grad_norm': 1.5149720907211304, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 88.2632, 'train_samples_per_second': 0.113, 'train_steps_per_second': 0.011, 'train_loss': 3.9634811878204346, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1, \n",
    "        gradient_accumulation_steps=10, \n",
    "        warmup_steps=1, \n",
    "        max_steps=1, \n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False \n",
    "\n",
    "try:\n",
    "  trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "  print(\"Key board interrupt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento do Modelo Finetunado\n",
    "\n",
    "Finalmente, para realizar inferências com o modelo finetunado, carregamos o modelo LoRA do Hub utilizando `from_pretrained`, passando tanto o modelo subjacente quanto o tokenizador. Em seguida, criamos um lote de entrada e geramos uma sequência de texto com o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"tayyibsupercool/bloom-560m-lora\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " I like strawberrys, but I don't like them in the garden. I don't like the smell of them in the garden. I don't like the smell of them in the garden. I don't like the smell of them in the garden. I don't like the smell of them\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cuda')\n",
    "batch = tokenizer(\"I like strawberrys\", return_tensors='pt').to('cuda')\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
